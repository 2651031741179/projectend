import os
import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.models import load_model
from tkinter import filedialog, Tk
from PIL import ImageFont, ImageDraw, Image
from gtts import gTTS
from playsound import playsound
import uuid

# --- ‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ---
FONT_PATH = r"THSarabunNew.ttf"

# ==========================================================
# ========== ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• (Hardcoded Path) ===============
# ==========================================================
model_path = 'eatrice.h5'
if not os.path.exists(model_path):
    print(f"‚ùå FATAL ERROR: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• '{model_path}'")
    exit()

model = load_model(model_path)
print(f"üü¢ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {model_path}")

# ======== ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå MP_Data ========
DATA_PATH = 'MP_Data'
if not os.path.exists(DATA_PATH):
    print(f"‚ùå ERROR: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå '{DATA_PATH}'")
    exit()

actions = np.array(sorted([d for d in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, d))]))
if len(actions) == 0:
    print("‚ùå ERROR: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÉ‡∏ô MP_Data")
    exit()
print("üü¢ ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î:", actions)

# --- [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç 1] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏µ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ß‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ (‡πÄ‡∏ä‡πà‡∏ô 100 ‡∏™‡∏µ) ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Index Error ‡∏ñ‡πâ‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• ---
colors = [(np.random.randint(100, 255), np.random.randint(100, 255), np.random.randint(100, 255)) for _ in range(100)]


# ==========================================================
# ========== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô TTS (‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢) ===================
# ==========================================================
def speak_thai(text):
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏ó‡∏¢ + ‡πÄ‡∏•‡πà‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ"""
    try:
        filename = f"tts_{uuid.uuid4()}.mp3"
        tts = gTTS(text=text, lang='th')
        tts.save(filename)
        playsound(filename)
        os.remove(filename)
    except Exception as e:
        print(f"‚ö†Ô∏è TTS ERROR: {e}")


# ==========================================================
# ========== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Mediapipe & Utils ====================
# ==========================================================
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

def put_thai_text(image, text, position, font_path=FONT_PATH, font_size=32, color=(255, 255, 255)):
    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(pil_image)
    try:
        font = ImageFont.truetype(font_path, font_size)
    except IOError:
        # print("‚ùå Font not found, using default font") # ‡∏õ‡∏¥‡∏î print ‡∏£‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
        font = ImageFont.load_default()
    
    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

def mediapipe_detection(image, model):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = model.process(image)
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image, results

def draw_styled_landmarks(image, results):
    if results.face_landmarks:
        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)
    if results.pose_landmarks:
        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
    if results.left_hand_landmarks:
        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    if results.right_hand_landmarks:
        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

def extract_keypoints(results):
    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros((33, 3))
    left_hand = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros((21, 3))
    right_hand = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros((21, 3))
    return np.concatenate([pose, left_hand, right_hand]).flatten()

def prob_viz(res, actions, input_frame, colors, is_active):
    output_frame = input_frame.copy()
    if is_active:
        for num, prob in enumerate(res):
            # --- [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç 2] ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ num ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô actions ‡πÅ‡∏•‡∏∞ colors ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ---
            if num < len(actions) and num < len(colors):
                cv2.rectangle(output_frame, (0, 60 + num * 40), (int(prob * 100), 90 + num * 40), colors[num], -1)
                output_frame = put_thai_text(output_frame, actions[num], (0, 65 + num * 40), font_size=20)
    return output_frame


# ==========================================================
# =================== Logic ‡∏à‡∏±‡∏ö‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á ======================
# ==========================================================
sequence = []
sentence = []
predictions = []
threshold = 0.7
current_frame_count = 0
CLEAR_TIMEOUT_FRAMES = 60
last_detection_frame = 0
res = [] # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô list ‡∏ß‡πà‡∏≤‡∏á

cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
if not cap.isOpened():
    print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ")
    exit()

with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ü‡∏£‡∏°‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ")
            break

        current_frame_count += 1
        frame = cv2.resize(frame, (640, 480))

        image, results = mediapipe_detection(frame, holistic)
        draw_styled_landmarks(image, results)

        keypoints = extract_keypoints(results)
        sequence.append(keypoints)
        sequence = sequence[-30:]

        is_new_detection = False
        is_prediction_run = False

        if len(sequence) == 30:
            res = model.predict(np.expand_dims(sequence, axis=0))[0]
            
            # --- [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç 3] ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error ‡∏´‡∏≤‡∏Å res ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ actions ---
            if len(res) > len(actions):
                # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏î‡πâ class ‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤ actions ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î res ‡∏ó‡∏¥‡πâ‡∏á
                res_to_use = res[:len(actions)]
            else:
                res_to_use = res

            predictions.append(np.argmax(res_to_use))
            is_prediction_run = True

            if np.unique(predictions[-10:])[0] == np.argmax(res_to_use):
                if res_to_use[np.argmax(res_to_use)] > threshold:
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö index ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ actions
                    idx = np.argmax(res_to_use)
                    if idx < len(actions):
                        predicted_word = actions[idx]

                        if len(sentence) > 0:
                            if predicted_word != sentence[-1]:
                                sentence.append(predicted_word)
                                is_new_detection = True
                                speak_thai(predicted_word)
                        else:
                            sentence.append(predicted_word)
                            is_new_detection = True
                            speak_thai(predicted_word)

            if len(sentence) > 5:
                sentence = sentence[-5:]

        # --- ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î ---
        if is_new_detection:
            last_detection_frame = current_frame_count
            is_viz_active = True
        elif current_frame_count - last_detection_frame < CLEAR_TIMEOUT_FRAMES and len(sentence) > 0:
            is_viz_active = True
        elif current_frame_count - last_detection_frame >= CLEAR_TIMEOUT_FRAMES and len(sentence) > 0:
            sentence = []
            is_viz_active = False
            print("üí° ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á")
        else:
            is_viz_active = False

        if is_prediction_run or current_frame_count < 30:
            # ‡∏™‡πà‡∏á res ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°)
            if len(res) > 0:
                image = prob_viz(res, actions, image, colors, is_viz_active)

        cv2.rectangle(image, (0, 440), (640, 480), (245, 117, 16), -1)

        display_text = ' '.join(sentence) if sentence else "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á..."
        image = put_thai_text(image, display_text, (3, 445), font_size=30)

        cv2.imshow('OpenCV Feed', image)
        if cv2.waitKey(10) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()